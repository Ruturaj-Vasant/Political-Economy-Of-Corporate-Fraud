# -*- coding: utf-8 -*-
"""Copy of Data_cleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IKdm1csSa91SokijHVa_roP1s_5NepkE

###**Previous Code**
"""

import os
import pandas as pd

"""Summary Compensation Table (SCT) extraction via XPath (core logic).

This module ports the XPath-first strategy from your legacy
scripts/SEC_Documents/Compiling_functions.py with small, well-documented
helpers and safe defaults.

Main entry points:
- extract_sct_tables_from_bytes(html_bytes) -> List[pd.DataFrame]
- extract_sct_tables_from_file(path) -> List[pd.DataFrame]

Notes
- We return a list of candidate tables; caller decides how to persist.
- Cleaning mirrors the legacy behavior: flatten headers, strip strings,
  drop empty columns, dedupe columns.
"""
from __future__ import annotations

from typing import List
from pathlib import Path

import pandas as pd
from lxml import html as LH  # type: ignore
import re


def _flatten_columns(df: pd.DataFrame) -> pd.DataFrame:
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = [
            " ".join([str(x) for x in col if "Unnamed" not in str(x)]).strip()
            for col in df.columns.values
        ]
    else:
        df.columns = [str(c).strip() for c in df.columns]
    return df


def _strip_cells(df: pd.DataFrame) -> pd.DataFrame:
    # map over all cells
    return df.map(lambda x: x.strip() if isinstance(x, str) else x)


def process_extracted_table(table_el) -> pd.DataFrame:
    """Convert an lxml <table> element into a cleaned DataFrame."""
    df_list = pd.read_html(LH.tostring(table_el))
    if not df_list:
        return pd.DataFrame()
    df = df_list[0]
    df = _flatten_columns(df)
    df = _strip_cells(df)
    df = df.dropna(axis=1, how="all")
    df = df.dropna(axis=0, how="all")
    # Remove duplicate column names (keep first)
    df = df.loc[:, ~df.columns.duplicated()]
    # Apply deterministic normalization to standardize and consolidate columns
    df = normalize_sct_dataframe(df)
    return df


def normalize_sct_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """Normalize SCT DataFrame headers and consolidate duplicate logical columns.

    - Detects when the first row contains real headers (generic/Unnamed columns case)
    - Standardizes common SCT column names (salary, bonus, stock_awards, etc.)
    - Consolidates duplicates by preferring series with more numeric content
    - Drops all-empty cols/rows and duplicate rows
    """
    if df.empty:
        return df

    # If the very first row is all-NaN, drop it early (junk row from read_html)
    if len(df) and df.iloc[0].isna().all():
        df = df.iloc[1:].reset_index(drop=True)

    # Determine if current columns look generic (e.g., Unnamed or plain digits)
    orig_columns = list(df.columns)
    cols_as_str = [str(c) for c in orig_columns]
    looks_generic = all(re.match(r"^(Unnamed.*|\d+)$", c) for c in cols_as_str)

    if looks_generic and len(df) > 0:
        # Promote first row to header text; keep mapping by original column labels
        header_row = df.iloc[0].astype(str).str.replace(r"\n|\r", "", regex=True).str.strip()
        header_row.index = pd.Index(orig_columns)
        data_df = df.iloc[1:].reset_index(drop=True)
        header_by_label = header_row
    else:
        # Use existing column labels as header texts
        data_df = df.reset_index(drop=True)
        ser = pd.Series([str(c) for c in data_df.columns], index=data_df.columns)
        header_by_label = ser

    # Drop columns that are entirely NaN and remove fully-empty first-column rows
    data_df = data_df.dropna(axis=1, how="all")
    if data_df.shape[1] == 0:
        return data_df
    # Ensure there's a first column to check for empties
    data_df = data_df[data_df.iloc[:, 0].notna()].reset_index(drop=True)

    # Drop symbol-only columns (e.g., columns with only '$', dashes, punctuation)
    def _is_symbol_only_col(s: pd.Series) -> bool:
        if s is None:
            return False
        vals = s.dropna().astype(str).str.strip().str.replace("\u00a0", "", regex=False)
        if vals.empty:
            return False  # already handled by dropna(all)
        # True if every non-null cell contains no letters or digits
        return bool((~vals.str.contains(r"[A-Za-z0-9]", regex=True)).all())

    keep_cols = []
    for c in list(data_df.columns):
        if _is_symbol_only_col(data_df[c]):
            continue
        keep_cols.append(c)
    if len(keep_cols) != len(data_df.columns):
        data_df = data_df[keep_cols]

    # Mapping for canonical SCT columns (aligned with AI pipeline expectations)
    column_mapping = {
        'salary': ['salary'],
        'bonus': ['bonus'],
        'stock_awards': ['stock awards', 'stock-awards'],
        'option_awards': ['option awards', 'option-awards'],
        'non_equity_incentive_plan': [
            'non-equity incentive plan compensation',
            'non-equity incentive',
            'non equity incentive',
        ],
        'pension_value': ['change in pension', 'pension value', 'deferred compensation compensation'],
        'all_other_compensation': ['all other compensation', 'all other comp'],
        'total': ['total salary and incentive compensation', 'total'],
        'year': ['year', 'fiscal year ended'],
        'name_position': ['name and principal position', 'name & principal position', 'principal position', 'name'],
    }

    def clean_string_for_matching(text: str) -> str:
        t = str(text).lower().strip()
        t = re.sub(r"\s*\([^)]*\)", "", t)  # drop parentheticals
        t = re.sub(r"[^a-z0-9 ]", "", t)      # keep alnum + spaces
        t = re.sub(r"\s+", " ", t).strip()
        return t.replace(" ", "")

    reverse_mapping = {}
    for std, variants in column_mapping.items():
        for v in variants:
            reverse_mapping[clean_string_for_matching(v)] = std

    def normalize_column_name(name: str) -> str:
        key = clean_string_for_matching(name)
        if key in reverse_mapping:
            return reverse_mapping[key]
        # Fallbacks
        if 'salary' in key:
            return 'salary'
        if 'bonus' in key:
            return 'bonus'
        if 'stockaward' in key:
            return 'stock_awards'
        if 'optionaward' in key:
            return 'option_awards'
        if 'incentive' in key:
            return 'non_equity_incentive_plan'
        if 'pension' in key:
            return 'pension_value'
        if 'othercomp' in key or 'allother' in key:
            return 'all_other_compensation'
        if 'total' in key:
            return 'total'
        if 'year' in key:
            return 'year'
        if 'nameprincipalposition' in key or 'principalposition' in key or key == 'name':
            return 'name_position'
        return str(name)

    # Consolidate columns by normalized name; prefer series with more numeric content
    final_cols: dict[str, pd.Series] = {}
    for col_label in list(data_df.columns):
        header_text = header_by_label.get(col_label, str(col_label))
        norm = normalize_column_name(header_text)
        series = data_df[col_label].reset_index(drop=True)

        if norm not in final_cols:
            final_cols[norm] = series
            continue

        def as_numeric(s: pd.Series) -> pd.Series:
            s = s.astype(str).str.replace('$', '', regex=False) \
                         .str.replace(',', '', regex=False) \
                         .str.replace('‚Äî', '', regex=False) \
                         .str.replace('(', '-', regex=False) \
                         .str.replace(')', '', regex=False) \
                         .str.strip()
            return pd.to_numeric(s, errors='coerce')

        existing = final_cols[norm]
        num_exist = as_numeric(existing)
        num_new = as_numeric(series)
        c_exist = num_exist.dropna().nunique()
        c_new = num_new.dropna().nunique()

        if c_new > c_exist:
            final_cols[norm] = series
        elif c_new == c_exist and c_new > 0:
            if series.notna().sum() > existing.notna().sum():
                final_cols[norm] = series
        elif existing.isna().all() and series.notna().any():
            final_cols[norm] = series

    cleaned = pd.DataFrame(final_cols)
    # Ensure unique columns and drop duplicate rows
    cleaned = cleaned.loc[:, ~cleaned.columns.duplicated()]
    cleaned.drop_duplicates(inplace=True)
    return cleaned


_XPATH_SCT_HEADER_TR = r"""
//tr[
  .//text()[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'name')]
  and (
    (
      .//text()[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'principal')]
      and .//text()[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'position')]
    )
    or (
      following-sibling::tr[1]//text()[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'principal')]
      and following-sibling::tr[1]//text()[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'position')]
    )
  )
]
"""


def _unique_tables(tr_nodes) -> List:
    seen = set()
    out = []
    for tr in tr_nodes:
        table = tr.getparent()
        while table is not None and getattr(table, "tag", None) != "table":
            table = table.getparent()
        if table is None:
            continue
        key = LH.tostring(table)[:200]  # cheap content hash key
        if key in seen:
            continue
        seen.add(key)
        out.append(table)
    return out


def extract_sct_tables_from_bytes(html_bytes: bytes) -> List[pd.DataFrame]:
    """Return candidate SCT tables as cleaned DataFrames (may be empty list)."""
    try:
        tree = LH.fromstring(html_bytes)
    except Exception:
        return []
    tr_nodes = tree.xpath(_XPATH_SCT_HEADER_TR)
    if not tr_nodes:
        return []
    tables = _unique_tables(tr_nodes)
    dfs: List[pd.DataFrame] = []
    for t in tables:
        try:
            df = process_extracted_table(t)
            if not df.empty:
                dfs.append(df)
        except Exception:
            continue
    return dfs


def extract_sct_tables_from_file(path: str | Path) -> List[pd.DataFrame]:
    p = Path(path)
    try:
        html_bytes = p.read_bytes()
    except Exception:
        return []
    return extract_sct_tables_from_bytes(html_bytes)

"""# **Reading the Files**

###**Extracting CSV from the HTML Tables**
"""

# import glob

# html_list = glob.glob('/content/*_SCT*.html')
# # html_list = glob.glob('/content/A_2013-02-06_SCT*.html')
# print(html_list)

"""Extracting the data from google drive"""

from google.colab import drive
drive.mount('/content/drive')

import glob
import pandas as pd

base_path = "/content/drive/MyDrive/data/"

# recursive search through all ticker folders
html_list = glob.glob(base_path + "**/DEF_14A/extracted/*_SCT*.html", recursive=True)

print(f"FOUND {len(html_list)} SCT HTML FILES\n")

html_tables = {}

for path in html_list[:20]:
    try:
        ticker = path.split("/")[-4]   # extract ticker folder name
        df_list = pd.read_html(path)
        combined = pd.concat(df_list, ignore_index=True)
        html_tables[(ticker, path)] = combined
        print(f"Loaded: {ticker} ‚Üí {path}")
    except Exception as e:
        print(f"FAILED: {path} ‚Üí {e}")

print("\nDONE.")

# for htmlfile in html_list[:1]:
#   print(htmlfile)
#   df = pd.read_html(htmlfile)
#   print(df)

"""###Combine HTML to one dataframe"""

def clean_combined_html_df(htmlfile):
    # extract tables
    tables = pd.read_html(htmlfile)
    combined = pd.concat(tables, ignore_index=True)

    # flatten multi-row headers into one line
    combined = _flatten_columns(combined)

    # strip whitespace in all cells
    combined = _strip_cells(combined)
    combined = combined.loc[:, ~combined.columns.duplicated()]
    # drop empty rows/cols early
    combined = combined.dropna(axis=0, how='all')
    combined = combined.dropna(axis=1, how='all')

    return combined

csv_tables = {}

for htmlfile in html_list[:20]:
    df_name = os.path.splitext(os.path.basename(htmlfile))[0]
    csv_tables[df_name] = clean_combined_html_df(htmlfile)
    print("processed:", df_name)

# print(csv_tables.values())

csv_dataframes = csv_tables.copy()
print(csv_dataframes.keys())

"""###Reading CSV"""

# files = os.listdir('/content')
# # print(files)
# csv_list = []

# for f in files:
#   if f.endswith('.csv'):
#     csv_list.append(f)

# print(csv_list)

# # import pandas as pd
# # import os

# # Get a list of all .csv files in the current directory
# csv_files = [f for f in os.listdir('/content') if f.endswith('.csv')]

# # Dictionary to store DataFrames
# csv_dataframes = {}

# # Read each CSV file into a pandas DataFrame and store it in the dictionary
# for file in csv_files:
#     file_path = os.path.join('/content', file)
#     df_name = os.path.splitext(file)[0] # Get filename without extension
#     csv_dataframes[df_name] = pd.read_csv(file_path)
#     print(f"Successfully loaded '{file}' into DataFrame '{df_name}'")

"""### Displaying the first 5 rows of each loaded DataFrame:"""

for df_name, df in csv_dataframes.items():
    print(f"\n--- DataFrame: {df_name} ---")
    # df = df.replace({$}as())) # This line caused a SyntaxError and has been commented out
    display(df.head())

"""###Clearing columns and remaiming n_a as 0"""

# import numpy as np
# for df_name, df in csv_dataframes_normalized.items():
#     df = df.replace("n_a", "0")

#     df = df.replace([""," "], np.nan)

#     # drop columns literally named 'nan' or empty string
#     # bad_cols = [c for c in df.columns if str(c).lower() in ("nan", "")]
#     # df = df.drop(columns=bad_cols, errors='ignore')

#     # drop completely empty cols/rows
#     df = df.dropna(axis=1, how='all')
#     df = df.dropna(axis=0, how='all')

#     # üî¥ IMPORTANT: write it back
#     csv_dataframes_normalized[df_name] = df
import numpy as np

# for df_name, df in csv_dataframes_normalized.items():
for df_name, df in csv_dataframes.items():

    # 1Ô∏è‚É£ convert placeholders to real values
    df = df.replace("n_a", np.nan)
    df = df.replace("0_0_0", "0")    # optional ‚Äì handles triple zeros
    df = df.replace("-", np.nan)     # optional ‚Äì SEC uses dash for NA

    # 2Ô∏è‚É£ convert blank strings to NaN
    df = df.replace(["", " "], np.nan)

    # 3Ô∏è‚É£ drop rows that are fully empty
    df = df.dropna(axis=0, how='all')

    # 4Ô∏è‚É£ drop columns that are fully empty
    df = df.dropna(axis=1, how='all')

    # 5Ô∏è‚É£ assign back
    # csv_dataframes_normalized[df_name] = df
    csv_dataframes[df_name] = df


    # print(f"Cleaned: {df_name} ‚Üí {df.shape}")

# for df_name, df in csv_dataframes_normalized.items():
#     print(f"\n--- Headers for DataFrame: {df_name} ---")
#     print(df.columns.tolist())

# # for df_name, df in csv_dataframes_normalized.items():
# for df_name, df in csv_dataframes.items():

#     print(f"\n--- cleaned DataFrame: {df_name} ---")
#     display(df)

"""### Displaying the first 5 rows of each loaded DataFrame:"""

for df_name, df in csv_dataframes.items():
    print(f"\n--- DataFrame: {df_name} ---")
    # df = df.replace({$}as())) # This line caused a SyntaxError and has been commented out
    display(df.head())

"""###Basic text cleaning"""

def clean_text_basic(raw: str) -> str:
    if raw is None:
        return ""

    s = str(raw)
    s = s.replace("\u00a0", " ")                     # non-breaking space
    s = re.sub(r"(\w)-\s+(\w)", r"\1\2", s)         # fix hyphen splits
    s = s.replace("/", " ")                         # slash spacing
    s = re.sub(r"\s+", " ", s).strip()              # collapse whitespace
    s = re.sub(r"\s*\([^)]*\)", "", s)              # footnotes
    s = s.replace("$", "")
    s = s.replace(",", "")
    s = re.sub(r"\s+", " ", s).strip().lower()

    if not s:
        return ""

    return s

def clean_basic_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    df2 = df.copy()
    for col in df2.columns:
        df2[col] = df2[col].apply(
            lambda x: clean_text_basic(x) if isinstance(x, str) else x
        )
    return df2

for name, df in csv_dataframes.items():

    # 1Ô∏è‚É£ basic cleaning
    df = clean_basic_dataframe(df)

    # overwrite original entry
    csv_dataframes[name] = df

    print(f"‚úî processed: {name}")

"""### Normalizaing the Headers"""

# @title
# import re
# import pandas as pd # Ensure pandas is imported if not already

# def normalize_sct_text(raw: str) -> str:
#     """Normalize any SCT-related text (headers or cells) to a stable key."""
#     if raw is None:
#         return ""

#     s = str(raw).replace("\u00a0", " ")
#     s = re.sub(r"(\w)-\s+(\w)", r"\1\2", s)
#     s = s.replace("/", " ")
#     s = re.sub(r"\s+", " ", s).strip()
#     if not s:
#         return ""

#     # Remove footnotes and currency markers.
#     s = re.sub(r"\s*\([^)]*\)", "", s)
#     s = s.replace("$", "")
#     s = s.replace(",", "")
#     s = re.sub(r"\s+", " ", s).strip().lower()

#     # Specific rules for common headers
#     if re.search(r"\bname\b|\bposition\b|name.*(and|_).*position|position.*(and|_).*name|principal.?position|name.?and", s):
#         return "name_position"
#     if "fiscal" in s or re.search(r"\byear\b", s):
#         return "fiscal_year"
#     if "salary" in s:
#         return "salary"
#     if "bonus" in s:
#         return "bonus"
#     if "other annual" in s:
#         return "other_annual_comp"
#     if "stock" in s and "award" in s:
#         return "stock_awards"
#     if "option" in s:
#         return "option_awards"
#     if "non-equity" in s or "non equity" in s or "incentive plan" in s:
#         return "non_equity_incentive"
#     if "pension" in s or "deferred compensation" in s:
#         return "pension_value"
#     if "all other" in s:
#         return "all_other_comp"
#     if "total" in s:
#         return "total"

#     # Fallback: snake_case for unknown text.
#     return re.sub(r"[^a-z0-9]+", "_", s).strip("_")

# def normalize_dataframe_text(df):
#     """Normalize headers + all string cells using normalize_sct_text()."""
#     temp_df = df.copy()
#     potential_header_row_index = -1

#     # First, find the row that contains 'name_position' as a cell value
#     # We need to normalize cell values to find this. Do this on a temporary copy.
#     temp_processed_cells_df = temp_df.apply(lambda col: col.map(lambda x: normalize_sct_text(x) if isinstance(x, str) else x))

#     for i, row in temp_processed_cells_df.iterrows():
#         if 'name_position' in row.values:
#             potential_header_row_index = i
#             break

#     if potential_header_row_index != -1:
#         # Use the identified row's original values (after normalization) as new columns
#         new_columns = [normalize_sct_text(col_val) for col_val in temp_df.iloc[potential_header_row_index]]
#         temp_df.columns = new_columns
#         # Drop rows before and including the new header row
#         temp_df = temp_df.iloc[potential_header_row_index + 1:].reset_index(drop=True)
#     else:
#         # If no 'name_position' in cells to define a header row, just normalize existing headers
#         temp_df.columns = [normalize_sct_text(c) for c in temp_df.columns]

#     # Now apply normalization to all remaining string cells in the DataFrame
#     # This includes the new header row (which became column names) and the data rows.
#     return temp_df.apply(lambda col: col.map(lambda x: normalize_sct_text(x) if isinstance(x, str) else x))

# @title
# import re
# import pandas as pd

# def normalize_sct_text(raw: str) -> str:
#     """Normalize any SCT-related text (headers or cells) to a stable key."""
#     if raw is None:
#         return ""

#     s = str(raw).replace("\u00a0", " ")
#     s = re.sub(r"(\w)-\s+(\w)", r"\1\2", s)
#     s = s.replace("/", " ")
#     s = re.sub(r"\s+", " ", s).strip()
#     if not s:
#         return ""

#     # Remove footnotes and currency markers.
#     s = re.sub(r"\s*\([^)]*\)", "", s)
#     s = s.replace("$", "")
#     s = s.replace(",", "")
#     s = re.sub(r"\s+", " ", s).strip().lower()

#     # Specific rules for common headers
#     if re.search(r"\bname\b|\bposition\b|principal.?position", s):
#         return "name_position"
#     if "fiscal" in s or "year" in s:
#         return "fiscal_year"
#     if "salary" in s:
#         return "salary"
#     if "bonus" in s:
#         return "bonus"
#     if "stock" in s and "award" in s:
#         return "stock_awards"
#     if "option" in s:
#         return "option_awards"
#     if "equity" in s or "incentive" in s:
#         return "non_equity_incentive"
#     if "pension" in s:
#         return "pension_value"
#     if "all other" in s:
#         return "all_other_comp"
#     if "total" in s:
#         return "total"

#     # Fallback
#     return re.sub(r"[^a-z0-9]+", "_", s).strip("_")


# def normalize_dataframe_text(df):
#     """Normalize multi-row SCT tables into a clean single-header table."""

#     # STEP 1 ‚Äî remove empty rows/cols early
#     df = df.dropna(axis=0, how='all')
#     df = df.dropna(axis=1, how='all')

#     # STEP 2 ‚Äî use first 3 rows to build header (works for 2006‚Äì2007)
#     header_block = df.iloc[:3]

#     # STEP 3 ‚Äî turn into strings & fill blanks
#     header_block = header_block.astype(str).replace("nan", "").fillna("")

#     # STEP 4 ‚Äî merge header rows horizontally
#     merged_headers = header_block.apply(
#         lambda col: " ".join(col.values).strip()
#     )

#     # STEP 5 ‚Äî normalize merged headers
#     normalized_headers = [
#         normalize_sct_text(h) for h in merged_headers
#     ]

#     # STEP 6 ‚Äî assign column names
#     df.columns = normalized_headers

#     # STEP 7 ‚Äî remove header rows from data
#     df = df.iloc[3:].reset_index(drop=True)

#     # STEP 8 ‚Äî normalize remaining string cells
#     df = df.apply(
#         lambda col: col.map(
#             lambda x: normalize_sct_text(x) if isinstance(x, str) else x
#         )
#     )

#     return df

# @title
# for df_name, df in csv_dataframes.items():
#     csv_dataframes[df_name] = normalize_dataframe_text(df)
#     print(f"\n--- Normalized DataFrame: {df_name} ---")
#     display(csv_dataframes[df_name].head())

def normalize_semantic_text(raw: str) -> str:
    if raw is None:
        return ""

    s = str(raw).lower().strip()

    # category detection
    if "name" in s and "position" in s:
        return "name_position"
    if "fiscal" in s or "year" in s:
        return "fiscal_year"
    if "salary" in s:
        return "salary"
    if "bonus" in s:
        return "bonus"
    if "other annual" in s:
        return "other_annual_comp"
    if "stock" in s and "award" in s:
        return "stock_awards"
    if "option" in s:
        return "option_awards"
    if "incentive" in s:
        return "non_equity_incentive"
    if "pension" in s:
        return "pension_value"
    if "all other" in s:
        return "all_other_comp"
    if "total" in s:
        return "total"

    # fallback snake_case
    s = re.sub(r"[^a-z0-9]+", "_", s)
    return s.strip("_")

def normalize_semantic_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    df2 = df.copy()

    # normalize column names
    df2.columns = [normalize_semantic_text(c) for c in df2.columns]

    # normalize cell values
    for col in df2.columns:
        df2[col] = df2[col].apply(
            lambda x: normalize_semantic_text(x) if isinstance(x, str) else x
        )

    return df2

for name, df in csv_dataframes.items():

    # 2Ô∏è‚É£ semantic category normalization
    df = normalize_semantic_dataframe(df)

    # overwrite original entry
    csv_dataframes[name] = df

    print(f"‚úî processed: {name}")

normalized_headers_after_text_normalization = set()

for df_name, df in csv_dataframes.items():
    normalized_headers_after_text_normalization.update(df.columns.tolist())

print("\n--- Summary of New Normalized Column Headers after Text Normalization ---")
print(sorted(list(normalized_headers_after_text_normalization)))

# found = False

# for df_name, df in csv_dataframes.items():
#   if "name_position" in df.columns:
#     found = True
#     print(f"DataFrame '{df_name}' contains the 'name_position' column.")
#     print("Column Index:", df.columns.get_loc("name_position"))
#     print("First Valid Index:", df['name_position'].first_valid_index())

# if not found:
#   print("No DataFrame contains the 'name_position' column.")
# else:
#     print(f"DataFrame '{df_name}' does not contain the 'name_position' column.")

csv_dataframes_normalized = csv_dataframes.copy()

"""###Promote row to header"""

def promote_row_to_header(df, keyword="name_position"):
    """
    Find the first row containing the keyword and promote it to header.
    Keeps data intact, deletes rows above header row.
    """
    import pandas as pd

    # convert to string for matching, safe handling of NaN
    match_df = df.astype(str).apply(lambda col: col.str.lower())

    # detect first row that contains keyword
    header_row_index = None
    for i, row in match_df.iterrows():
        if keyword in row.values:
            header_row_index = i
            break

    # if no row contains keyword, return unchanged
    if header_row_index is None:
        return df.copy()

    # extract header names
    new_cols = df.iloc[header_row_index].astype(str).tolist()

    # apply header + trim data
    trimmed_df = df.iloc[header_row_index + 1:].copy()
    trimmed_df.columns = new_cols

    return trimmed_df.reset_index(drop=True)

for name, df in csv_dataframes.items():
    new_df = promote_row_to_header(df, keyword="name_position")
    csv_dataframes[name] = new_df

"""### Displaying the first 5 rows of each loaded DataFrame:"""

for df_name, df in csv_dataframes.items():
    print(f"\n--- DataFrame: {df_name} ---")
    # df = df.replace({$}as())) # This line caused a SyntaxError and has been commented out
    display(df.head())

"""### Locate and Trim DataFrames

### Subtask:
For each DataFrame in `csv_dataframes`, identify the index of the `name_position` column and the first row where a non-NaN value appears in that column. Then, trim the DataFrame to include only data from that row and column onwards.

**Reasoning**:
The subtask requires iterating through each DataFrame, identifying the 'name_position' column, finding the first non-NaN value in it, and then trimming the DataFrame from that row and column onwards. The provided code implements this logic by using `first_valid_index()` for rows and `get_loc()` for columns, then slicing the DataFrame accordingly.
"""

for df_name, df in csv_dataframes.items():
    if 'name_position' in df.columns:
        first_valid_row_index = df['name_position'].first_valid_index()
        if first_valid_row_index is not None:
            # Get the positional index of the first 'name_position' column
            name_position_col_index = list(df.columns).index('name_position')

            # Trim rows from first_valid_row_index onwards
            trimmed_df = df.loc[first_valid_row_index:].copy()

            # Trim columns from name_position_col_index onwards
            trimmed_df = trimmed_df.iloc[:, name_position_col_index:].copy()

            csv_dataframes[df_name] = trimmed_df
            print(f"DataFrame '{df_name}' trimmed successfully. First 5 rows:\n")
            display(csv_dataframes[df_name].head())
        else:
            print(f"DataFrame '{df_name}': 'name_position' column contains no valid non-NaN values. Skipping trimming.\n")
    else:
        print(f"DataFrame '{df_name}' does not contain 'name_position' column. Skipping trimming.\n")

"""**Reasoning**:
The `TypeError` arises because `df.columns.get_loc('name_position')` can return a boolean array (if multiple columns are named 'name_position') or a slice, which `iloc` does not directly support for column indexing in the format `iloc[:, boolean_array]`. To fix this, I need to ensure `name_position_col_index` is always a single integer representing the positional index of the first occurrence of the 'name_position' column. Using `list(df.columns).index('name_position')` achieves this reliably.

### Displaying the first 5 rows of each loaded DataFrame:
"""

for df_name, df in csv_dataframes.items():
    print(f"\n--- DataFrame: {df_name} ---")
    # df = df.replace({$}as())) # This line caused a SyntaxError and has been commented out
    display(df.head())

"""## Task
Locate the `name_position` column in each DataFrame within `csv_dataframes` and identify the first row with a non-NaN value in that column. Trim each DataFrame to retain only data from that identified row and `name_position` column onwards. Finally, display the first 5 rows of each trimmed DataFrame.

## Display Trimmed DataFrames

### Subtask:
Display the first 5 rows of each DataFrame after trimming to confirm the data has been correctly isolated.
"""

for df_name, df in csv_dataframes.items():
    print(f"\n--- Trimmed DataFrame: {df_name} ---")
    display(df.head())

"""##Copying the dataframe list for post processing

###fix split headers

it merges columns with same name
"""

def fix_two_row_header(df):
    first_row = df.iloc[0].astype(str).str.lower().tolist()
    col_names = df.columns.astype(str).str.lower().tolist()

    matches = sum([1 for x in first_row if x in col_names])
    if matches < 2:
        return df

    new_cols = []
    for old, new in zip(df.columns, df.iloc[0]):

        new = str(new).strip().lower()
        old = str(old).strip().lower()

        # ---- RULE 1: if new == old ‚Üí use one version only ----
        if new == old:
            new_cols.append(old)
            continue

        # ---- RULE 2: keep non-empty replacement logic ----
        if new in ("nan", "", None):
            new_cols.append(old)
            continue

        if old in ("nan", "", None):
            new_cols.append(new)
            continue

        # ---- RULE 3: avoid doubling patterns like "salary_salary" ----
        if new in old:
            new_cols.append(old)
            continue

        if old in new:
            new_cols.append(new)
            continue

        # ---- RULE 4: safe merge fallback ----
        new_cols.append(f"{old}_{new}")

    df.columns = new_cols
    df = df.iloc[1:].reset_index(drop=True)

    return df

for name, df in csv_dataframes_normalized.items():
    fixed = fix_two_row_header(df.copy())
    csv_dataframes_normalized[name] = fixed
    print(f"Repaired header split in: {name}")

for df_name, df in csv_dataframes_normalized.items():
    print(f"\n--- cleaned DataFrame: {df_name} ---")
    display(df)

def merge_duplicate_cols(df):
    # convert blanks to NaN
    df = df.replace(["", " "], np.nan)

    merged = {}

    for col in df.columns.unique():
        subset = df.filter(regex=f"^{col}$")
        merged[col] = subset.bfill(axis=1).iloc[:, 0]

    return pd.DataFrame(merged)

for df_name, df in csv_dataframes_normalized.items():
    df = merge_duplicate_cols(df)
    csv_dataframes_normalized[df_name] = df

"""###Expand the _ column"""

# from pandas.api.types import is_numeric_dtype

# expanded_tables = {}

# for df_name, df in csv_dataframes_normalized.items():

#     if "fiscal_year" not in df.columns:
#         expanded_tables[df_name] = df.copy()
#         continue

#     new_rows = []

#     for _, row in df.iterrows():

#         fiscal_years = str(row["fiscal_year"]).split("_")
#         n = len(fiscal_years)

#         for i in range(n):
#             new_row = {}

#             # keep name_position as-is (repeat, don't split)
#             new_row["name_position"] = row["name_position"]

#             # assign fiscal year value
#             new_row["fiscal_year"] = fiscal_years[i]

#             # expand remaining columns
#             for col in df.columns:
#                 if col in ["name_position", "fiscal_year"]:
#                     continue

#                 val = row[col]

#                 # if entry is "123_456_789"
#                 if isinstance(val, str) and "_" in val:
#                     parts = val.split("_")
#                     new_row[col] = parts[i] if i < len(parts) else None

#                 else:
#                     # replicate value across expanded rows
#                     new_row[col] = val

#             new_rows.append(new_row)

#     expanded_tables[df_name] = pd.DataFrame(new_rows)

# expanded_tables = {}

# for name, df in csv_dataframes_normalized.items():
#     expanded_tables[name] = expand_sct_smart(df)
#     print(f"{name}: {len(df)} ‚Üí {len(expanded_tables[name])}")

# for df_name, df in expanded_tables.items():
#     print(f"\n--- cleaned DataFrame: {df_name} ---")
#     display(df)

for df_name, df in csv_dataframes_normalized.items():
    print(f"\n--- cleaned DataFrame: {df_name} ---")
    display(df)