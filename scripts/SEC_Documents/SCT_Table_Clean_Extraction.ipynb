{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCT Table Clean Extraction (No LLM)\n",
    "\n",
    "Deterministic, step-by-step extraction of Summary Compensation Tables (SCT) from HTML into clean CSV/Parquet.\n",
    "\n",
    "What this notebook does:\n",
    "- Discover HTML tables and select the best SCT candidate via keyword scoring.\n",
    "- Detect the header row, flatten headers, and normalize header names (drop footnotes, NBSP, extra spaces).\n",
    "- Drop placeholder columns (currency symbols, em-dashes) and deduplicate repeated columns.\n",
    "- Map columns to canonical fields (salary, bonus, stock_awards, option_awards, non_equity_incentive, pension_value, all_other_comp, total, year, name_position).\n",
    "- Coerce numbers (parentheses→negative, strip $ and commas), split name/position, and set dtypes.\n",
    "- Validate row totals and add quality flags (total_calc/total_diff/total_ok).\n",
    "- Save CSV and Parquet to the corresponding `extracted` folder.\n",
    "\n",
    "Optional at the end:\n",
    "- Tidy long format (one row per executive×year) as a separate cell you can enable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ef3d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "# lxml HTML parser for XPath selection (match Table_format.ipynb approach)\n",
    "from lxml import html\n",
    "\n",
    "# Ensure Parquet support (required as per your preference)\n",
    "import pyarrow  # noqa: F401  # should be installed; used by DataFrame.to_parquet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57edd0a5",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "- Choose a ticker and form folder.\n",
    "- The notebook will list HTML files found and let you pick by index, or you can process all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a8c3a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 HTML files in data/ABCP/DEF_14A\n",
      "Using HTML_PATH = data/ABCP/DEF_14A/2013-04-01_DEF_14A.html\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path('data')\n",
    "TICKER = 'ABCP'\n",
    "FORM = 'DEF_14A'\n",
    "\n",
    "# Default file (you can change this or select by index from HTML_FILES below)\n",
    "DEFAULT_HTML = BASE_DIR / TICKER / FORM / '2013-04-01_DEF_14A.html'\n",
    "\n",
    "# Discover all HTML files under the chosen ticker/form\n",
    "FORM_DIR = BASE_DIR / TICKER / FORM\n",
    "HTML_FILES: List[Path] = sorted(FORM_DIR.glob('*.html')) if FORM_DIR.exists() else []\n",
    "print(f'Found {len(HTML_FILES)} HTML files in {FORM_DIR}')\n",
    "for i, p in enumerate(HTML_FILES[:25]):\n",
    "    print(f'  [{i}]', p.name)\n",
    "\n",
    "# Select by index if you want; -1 means use DEFAULT_HTML\n",
    "SELECT_INDEX = -1\n",
    "HTML_PATH = (HTML_FILES[SELECT_INDEX] if (0 <= SELECT_INDEX < len(HTML_FILES)) else DEFAULT_HTML)\n",
    "print('Using HTML_PATH =', HTML_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07fd254",
   "metadata": {},
   "source": [
    "## Canonical field mapping and helpers\n",
    "We define canonical output fields and token sets to map messy headers to the canonical names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bbcb78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANON_ORDER = [\n",
    "    'executive_name', 'position', 'year',\n",
    "    'salary', 'bonus', 'stock_awards', 'option_awards',\n",
    "    'non_equity_incentive', 'pension_value', 'all_other_comp', 'total',\n",
    "]\n",
    "\n",
    "KEY_TOKENS: Dict[str, List[str]] = {\n",
    "    'salary': ['salary'],\n",
    "    'bonus': ['bonus'],\n",
    "    'stock_awards': ['stock awards', 'stock-awards'],\n",
    "    'option_awards': ['option awards', 'option-awards'],\n",
    "    'non_equity_incentive': ['non-equity incentive', 'non equity incentive'],\n",
    "    'pension_value': ['change in pension', 'pension value', 'deferred compensation earnings'],\n",
    "    'all_other_comp': ['all other compensation'],\n",
    "    'total': ['total'],\n",
    "    'year': ['year', 'fiscal year'],\n",
    "    'name_position': ['name and principal position', 'name & principal position', 'principal position', 'name'],\n",
    "}\n",
    "\n",
    "PLACEHOLDER_HEADERS = {'', '$', '—', '–', '-'}\n",
    "\n",
    "def normalize_header(h: str) -> str:\n",
    "    s = re.sub(r'\\s+', ' ', str(h)).strip()\n",
    "    s = re.sub(r'\\([^)]*\\)', '', s)  # drop footnotes like (c)(2), ($)\n",
    "    s = s.replace(' ', ' ')        # NBSP\n",
    "    s = re.sub(r'\\s+', ' ', s).strip().lower()\n",
    "    return s\n",
    "\n",
    "def canonical_for(col: str) -> Optional[str]:\n",
    "    c = normalize_header(col)\n",
    "    # strong check for name+position composites\n",
    "    if 'name' in c and 'position' in c:\n",
    "        return 'name_position'\n",
    "    for key, toks in KEY_TOKENS.items():\n",
    "        for t in toks:\n",
    "            if t in c:\n",
    "                return key\n",
    "    return None\n",
    "\n",
    "def is_placeholder_col(sr: pd.Series) -> bool:\n",
    "    vals = sr.dropna().astype(str).str.strip().str.replace(' ', ' ', regex=False)\n",
    "    if vals.empty:\n",
    "        return True\n",
    "    return vals.str.fullmatch(r'(\\$)?|—|–|-').all()\n",
    "\n",
    "def to_number(x):\n",
    "    s = str(x).strip().replace(' ', ' ')\n",
    "    if s in ('', '-', '–', '—'):\n",
    "        return pd.NA\n",
    "    s = re.sub(r'[,$]', '', s)\n",
    "    m = re.fullmatch(r'\\((.*)\\)', s)\n",
    "    if m:\n",
    "        s = '-' + m.group(1)\n",
    "    try:\n",
    "        v = float(s)\n",
    "        return int(v) if v.is_integer() else v\n",
    "    except Exception:\n",
    "        return pd.NA\n",
    "\n",
    "def report_date_from_filename(p: Path) -> Optional[str]:\n",
    "    m = re.match(r'(\\d{4}-\\d{2}-\\d{2})_', p.stem)\n",
    "    return m.group(1) if m else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17ea29",
   "metadata": {},
   "source": [
    "## Select SCT table via XPath (same logic as Table_format.ipynb)\n",
    "Locate the header row containing 'name', 'principal', and 'position', ascend to its table, then parse that table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b84af70",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/ABCP/DEF_14A/2013-04-01_DEF_14A.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m         table = table.getparent()\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m table\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m tbl_el = \u001b[43mfind_sct_table_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHTML_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mFound SCT table element?\u001b[39m\u001b[33m'\u001b[39m , tbl_el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tbl_el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mfind_sct_table_element\u001b[39m\u001b[34m(html_path)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfind_sct_table_element\u001b[39m(html_path: Path):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     content = \u001b[43mhtml_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mignore\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     tree = html.fromstring(content)\n\u001b[32m      4\u001b[39m     xpath_expr = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33m//tr[\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33m  .//text()[contains(translate(., \u001b[39m\u001b[33m'\u001b[39m\u001b[33mABCDEFGHIJKLMNOPQRSTUVWXYZ\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mabcdefghijklmnopqrstuvwxyz\u001b[39m\u001b[33m'\u001b[39m\u001b[33m), \u001b[39m\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)]\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m \u001b[33m]\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/pathlib.py:1027\u001b[39m, in \u001b[36mPath.read_text\u001b[39m\u001b[34m(self, encoding, errors)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[33;03mOpen the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[32m   1025\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1026\u001b[39m encoding = io.text_encoding(encoding)\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   1028\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/pathlib.py:1013\u001b[39m, in \u001b[36mPath.open\u001b[39m\u001b[34m(self, mode, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1012\u001b[39m     encoding = io.text_encoding(encoding)\n\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/ABCP/DEF_14A/2013-04-01_DEF_14A.html'"
     ]
    }
   ],
   "source": [
    "def find_sct_table_element(html_path: Path):\n",
    "    content = html_path.read_text(encoding='utf-8', errors='ignore')\n",
    "    tree = html.fromstring(content)\n",
    "    xpath_expr = \"\"\"\n",
    "//tr[\n",
    "  .//text()[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'name')]\n",
    "  and .//text()[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'principal')]\n",
    "  and .//text()[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'position')]\n",
    "]\n",
    "\"\"\"\n",
    "    tr_nodes = tree.xpath(xpath_expr)\n",
    "    if not tr_nodes:\n",
    "        return None\n",
    "    table = tr_nodes[0].getparent()\n",
    "    while table is not None and getattr(table, 'tag', None) != 'table':\n",
    "        table = table.getparent()\n",
    "    return table\n",
    "\n",
    "tbl_el = find_sct_table_element(HTML_PATH)\n",
    "print('Found SCT table element?' , tbl_el is not None)\n",
    "if tbl_el is not None:\n",
    "    preview = html.tostring(tbl_el)[:500]\n",
    "    print('Table preview (first 500 bytes):')\n",
    "    print(preview)\n",
    "    df_raw = pd.read_html(html.tostring(tbl_el))[0]\n",
    "    display(df_raw.head(8))\n",
    "else:\n",
    "    raise RuntimeError('No SCT header row found via XPath; adjust selection or pick another file.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6291cebe",
   "metadata": {},
   "source": [
    "## Detect header row and flatten columns\n",
    "Choose a header row within the first few rows, then flatten MultiIndex headers if any.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab228219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_header_row(df: pd.DataFrame, max_rows: int = 6) -> pd.DataFrame:\n",
    "    for i in range(min(max_rows, len(df))):\n",
    "        row_low = df.iloc[i].astype(str).str.lower()\n",
    "        hits = 0\n",
    "        joined = ' '.join(list(row_low))\n",
    "        for toks in KEY_TOKENS.values():\n",
    "            if any(tok in joined for tok in toks):\n",
    "                hits += 1\n",
    "        if hits >= 2:\n",
    "            df2 = df.copy()\n",
    "            df2.columns = df2.iloc[i].astype(str).tolist()\n",
    "            df2 = df2.iloc[i+1:].reset_index(drop=True)\n",
    "            return df2\n",
    "    return df\n",
    "\n",
    "def flatten_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        cols = [\n",
    "            ' '.join(str(x) for x in tup if (str(x) and 'unnamed' not in str(x).lower())).strip()\n",
    "            for tup in df.columns.to_list()\n",
    "        ]\n",
    "        df.columns = cols\n",
    "    else:\n",
    "        df.columns = [str(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "step_df = detect_header_row(df_raw)\n",
    "step_df = flatten_columns(step_df)\n",
    "print('Columns after header detection/flatten (first 20):', list(step_df.columns)[:20])\n",
    "step_df.head(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020ce55b",
   "metadata": {},
   "source": [
    "## Normalize headers and drop placeholder columns\n",
    "Lowercase, strip footnotes and NBSPs, drop currency/emdash columns, deduplicate exact duplicates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e244593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_headers_and_drop_placeholders(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    df2 = df.dropna(how='all').reset_index(drop=True).copy()\n",
    "    original_cols = list(df2.columns)\n",
    "    df2.columns = [normalize_header(c) for c in df2.columns]\n",
    "    dropped: List[str] = []\n",
    "    for c in list(df2.columns):\n",
    "        if c in PLACEHOLDER_HEADERS:\n",
    "            dropped.append(c)\n",
    "            continue\n",
    "        if is_placeholder_col(df2[c]):\n",
    "            dropped.append(c)\n",
    "    df2 = df2.drop(columns=list(set(dropped)), errors='ignore')\n",
    "    # de-duplicate exact duplicates\n",
    "    df2 = df2.loc[:, ~df2.columns.duplicated()]\n",
    "    return df2, dropped\n",
    "\n",
    "norm_df, dropped_cols = normalize_headers_and_drop_placeholders(step_df)\n",
    "print('Dropped placeholder columns:', dropped_cols)\n",
    "print('Columns after normalization (first 20):', list(norm_df.columns)[:20])\n",
    "norm_df.head(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470c1cd5",
   "metadata": {},
   "source": [
    "## Select best columns per canonical metric\n",
    "Map headers to canonical fields and keep one column per metric (the one with the most numeric-looking values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a9f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_score(sr: pd.Series) -> int:\n",
    "    return pd.to_numeric(sr.astype(str)\n",
    "                         .str.replace(r'[\\$,]', '', regex=True)\n",
    "                         .str.replace(r'\\s', '', regex=True)\n",
    "                         .str.replace(r'^\\((.*)\\)$', r'-\\1', regex=True),\n",
    "                         errors='coerce').notna().sum()\n",
    "\n",
    "def select_best_columns(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, str]]:\n",
    "    groups: Dict[str, List[str]] = {}\n",
    "    for c in df.columns:\n",
    "        key = canonical_for(c)\n",
    "        if key:\n",
    "            groups.setdefault(key, []).append(c)\n",
    "    keep: Dict[str, str] = {}  # canonical -> original column name\n",
    "    for key, cols in groups.items():\n",
    "        if key in ('executive_name', 'position'):\n",
    "            continue\n",
    "        best = max(cols, key=lambda x: numeric_score(df[x])) if cols else None\n",
    "        if best:\n",
    "            keep[key] = best\n",
    "    if 'year' in groups:\n",
    "        keep['year'] = max(groups['year'], key=lambda x: numeric_score(df[x]))\n",
    "    if 'name_position' in groups:\n",
    "        keep['name_position'] = groups['name_position'][0]\n",
    "    sel = df[list(keep.values())].copy() if keep else df.copy()\n",
    "    sel.columns = list(keep.keys()) if keep else sel.columns\n",
    "    return sel, keep\n",
    "\n",
    "sel_df, kept_map = select_best_columns(norm_df)\n",
    "print('Kept columns (canonical -> original):')\n",
    "for k, v in kept_map.items():\n",
    "    print(f'  {k:22s} <- {v}')\n",
    "sel_df.head(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1746dae",
   "metadata": {},
   "source": [
    "## Coerce numbers, split name/position, order columns\n",
    "Convert numeric fields, split the composite name/position, and re-order columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_fields(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # split name/position\n",
    "    if 'name_position' in out.columns:\n",
    "        parts = out['name_position'].astype(str).str.split(',', n=1)\n",
    "        out['executive_name'] = parts.str[0].str.strip()\n",
    "        out['position'] = parts.str[1].str.strip() if parts.apply(lambda x: len(x) > 1).any() else ''\n",
    "        out = out.drop(columns=['name_position'])\n",
    "    # year\n",
    "    if 'year' in out.columns:\n",
    "        out['year'] = pd.to_numeric(out['year'], errors='coerce').astype('Int64')\n",
    "    # numerics\n",
    "    for k in ['salary','bonus','stock_awards','option_awards','non_equity_incentive','pension_value','all_other_comp','total']:\n",
    "        if k in out.columns:\n",
    "            out[k] = out[k].map(to_number)\n",
    "    # order\n",
    "    cols = [c for c in CANON_ORDER if c in out.columns]\n",
    "    cols += [c for c in out.columns if c not in cols]\n",
    "    return out[cols]\n",
    "\n",
    "final_df = finalize_fields(sel_df)\n",
    "print(final_df.dtypes)\n",
    "final_df.head(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34834a6",
   "metadata": {},
   "source": [
    "## Quality checks (totals)\n",
    "Compute total_calc and validate against provided total (tolerance=5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e771aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_quality_flags(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    required = ['salary','bonus','stock_awards','option_awards','non_equity_incentive','pension_value','all_other_comp']\n",
    "    out = df.copy()\n",
    "    if 'total' in out.columns:\n",
    "        for k in required:\n",
    "            if k not in out.columns:\n",
    "                out[k] = 0\n",
    "        base_num = out[required].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        out['total_calc'] = base_num.sum(axis=1)\n",
    "        out['total_diff'] = (pd.to_numeric(out.get('total'), errors='coerce') - out['total_calc']).abs()\n",
    "        out['total_ok'] = (out['total_diff'] <= 5)\n",
    "    return out\n",
    "\n",
    "checked_df = add_quality_flags(final_df)\n",
    "checked_df.head(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce46cb6f",
   "metadata": {},
   "source": [
    "### Rows with total mismatch (if any)\n",
    "Helps identify residual misalignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ed8b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch = checked_df[checked_df.get('total_ok') == False]  # noqa: E712\n",
    "mismatch.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddb3f9f",
   "metadata": {},
   "source": [
    "## Save clean outputs (CSV + Parquet)\n",
    "Files will be written next to the source under the `extracted` folder with suffix `_SCT.clean`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e579901",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = TICKER\n",
    "date = report_date_from_filename(HTML_PATH) or 'UNKDATE'\n",
    "out_dir = (HTML_PATH.parent / 'extracted')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = out_dir / f'{ticker}_{date}_SCT.clean.csv'\n",
    "pq_path = out_dir / f'{ticker}_{date}_SCT.clean.parquet'\n",
    "\n",
    "checked_df.to_csv(csv_path, index=False)\n",
    "checked_df.to_parquet(pq_path, engine='pyarrow', index=False)\n",
    "\n",
    "print('Wrote:')\n",
    "print('  CSV   ', csv_path)\n",
    "print('  Parquet', pq_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2730e1fa",
   "metadata": {},
   "source": [
    "## (Optional) Batch process all HTML files for this ticker/form\n",
    "Process all files in the folder using the same cleaning pipeline and write clean CSV/Parquet for each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d595aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one(html_path: Path) -> Tuple[Optional[Path], Optional[Path]]:\n",
    "    try:\n",
    "        tbl_el = find_sct_table_element(html_path)\n",
    "        if tbl_el is None:\n",
    "            return None, None\n",
    "        df_raw = pd.read_html(html.tostring(tbl_el))[0]\n",
    "        step_df = flatten_columns(detect_header_row(df_raw))\n",
    "        norm_df, _ = normalize_headers_and_drop_placeholders(step_df)\n",
    "        sel_df, _ = select_best_columns(norm_df)\n",
    "        final_df = finalize_fields(sel_df)\n",
    "        checked_df = add_quality_flags(final_df)\n",
    "        t = html_path.parent.parent.name  # ticker from path data/<TICKER>/<FORM>/file.html\n",
    "        d = report_date_from_filename(html_path) or 'UNKDATE'\n",
    "        out_dir = html_path.parent / 'extracted'\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        csv_path = out_dir / f'{t}_{d}_SCT.clean.csv'\n",
    "        pq_path = out_dir / f'{t}_{d}_SCT.clean.parquet'\n",
    "        checked_df.to_csv(csv_path, index=False)\n",
    "        checked_df.to_parquet(pq_path, engine='pyarrow', index=False)\n",
    "        return csv_path, pq_path\n",
    "    except Exception as e:\n",
    "        print('Failed:', html_path.name, '->', e)\n",
    "        return None, None\n",
    "\n",
    "# Set RUN_BATCH = True to process all HTML files listed earlier\n",
    "RUN_BATCH = False\n",
    "if RUN_BATCH:\n",
    "    ok = 0; total = 0\n",
    "    for p in HTML_FILES:\n",
    "        total += 1\n",
    "        csvp, pqp = process_one(p)\n",
    "        if csvp is not None and pqp is not None:\n",
    "            ok += 1\n",
    "    print(f'Batch complete: {ok}/{total} files cleaned.')\n",
    "else:\n",
    "    print('Batch disabled. Set RUN_BATCH = True to enable.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Tidy long format (one row per executive×year)\n",
    "Set `RUN_TIDY = True` to generate a long-format view and optionally save it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_TIDY = False\n",
    "if RUN_TIDY:\n",
    "    df0 = checked_df.copy()\n",
    "    id_cols = [c for c in ['executive_name','position'] if c in df0.columns]\n",
    "    val_cols = [c for c in ['salary','bonus','stock_awards','option_awards','non_equity_incentive','pension_value','all_other_comp','total'] if c in df0.columns]\n",
    "    if 'year' in df0.columns:\n",
    "        # Already per-year rows in most SCTs; this just ensures tidy format\n",
    "        tidy = df0[id_cols + ['year'] + val_cols].copy()\n",
    "    else:\n",
    "        tidy = df0[id_cols + val_cols].copy()\n",
    "    display(tidy.head(10))\n",
    "    tidy_csv = out_dir / f'{ticker}_{date}_SCT.clean.tidy.csv'\n",
    "    tidy_pq = out_dir / f'{ticker}_{date}_SCT.clean.tidy.parquet'\n",
    "    tidy.to_csv(tidy_csv, index=False)\n",
    "    tidy.to_parquet(tidy_pq, engine='pyarrow', index=False)\n",
    "    print('Wrote tidy:')\n",
    "    print('  CSV   ', tidy_csv)\n",
    "    print('  Parquet', tidy_pq)\n",
    "else:\n",
    "    print('Tidy step disabled. Set RUN_TIDY = True to enable.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
